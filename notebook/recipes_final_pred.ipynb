{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# pip install keras==3.4.1\n",
        "!pip install openpyxl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkHGxe0EcQwZ",
        "outputId": "35074259-06a4-4996-84fa-a0d1bb91805d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openpyxl\n",
            "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting et-xmlfile (from openpyxl)\n",
            "  Downloading et_xmlfile-1.1.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.9/250.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: et-xmlfile, openpyxl\n",
            "Successfully installed et-xmlfile-1.1.0 openpyxl-3.1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYCDMRcNYMKH",
        "outputId": "877c0b7c-8115-46ea-dcc7-b097cb205dc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "from google.colab import drive\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3SaSm7faY5Sn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UnitStandardizer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        # Dictionary for standardizing units and descriptors\n",
        "        self.standard_units = {\n",
        "            'pounds': 'lb', 'pound': 'lb', 'lbs': 'lb', 'lb': 'lb',\n",
        "            'quarts': 'qt', 'quart': 'qt', 'qts': 'qt', 'qt': 'qt',\n",
        "            'cups': 'cup', 'cup': 'cup',\n",
        "            'pints': 'pint', 'pint': 'pint',\n",
        "            'gallons': 'gal', 'gallon': 'gal', 'gal': 'gal',\n",
        "            'ounces': 'oz', 'ounce': 'oz', 'ozs': 'oz', 'oz': 'oz',\n",
        "            'dozens': 'dozen', 'dozen': 'dozen',\n",
        "            'tablespoons': 'tbsp', 'tablespoon': 'tbsp', 'tbsps': 'tbsp', 'tbsp': 'tbsp',\n",
        "            'teaspoons': 'tsp', 'teaspoon': 'tsp', 'tsps': 'tsp', 'tsp': 'tsp',\n",
        "            'loaves': 'loaf', 'loaf': 'loaf',\n",
        "            'brownies': 'brownie', 'brownie': 'brownie',\n",
        "            'meatloaves': 'meatloaf', 'meatloaf': 'meatloaf',\n",
        "            'chimichangas': 'chimichanga', 'chimichanga': 'chimichanga',\n",
        "            'pizzas': 'pizza', 'pizza': 'pizza',\n",
        "            'crepes': 'crepe', 'crepe': 'crepe',\n",
        "            'truffles': 'truffle', 'truffle': 'truffle',\n",
        "            'rolls': 'roll', 'roll': 'roll',\n",
        "            'pancakes': 'pancake', 'pancake': 'pancake',\n",
        "            'muffins': 'muffin', 'muffin': 'muffin',\n",
        "            'cookies': 'cookie', 'cookie': 'cookie',\n",
        "            'cupcakes': 'cupcake', 'cupcake': 'cupcake',\n",
        "            'bars': 'bar', 'bar': 'bar',\n",
        "            'waffles': 'waffle', 'waffle': 'waffle',\n",
        "            'meatballs': 'meatball', 'meatball': 'meatball',\n",
        "            'biscuits': 'biscuit', 'biscuit': 'biscuit',\n",
        "            'appetizers': 'appetizer', 'appetizer': 'appetizer',\n",
        "            'breadsticks': 'breadstick', 'breadstick': 'breadstick',\n",
        "            'buns': 'bun', 'bun': 'bun',\n",
        "            'enchiladas': 'enchilada', 'enchilada': 'enchilada',\n",
        "            'desserts': 'dessert', 'dessert': 'dessert',\n",
        "            'servings': 'serving', 'serving': 'serving',\n",
        "            'strawberries': 'strawberry', 'strawberry': 'strawberry',\n",
        "            'quiches': 'quiche', 'quiche': 'quiche',\n",
        "            'omelettes': 'omelette', 'omelette': 'omelette',\n",
        "            'sandwiches': 'sandwich', 'sandwich': 'sandwich',\n",
        "            'pretzels': 'pretzel', 'pretzel': 'pretzel',\n",
        "            'mushrooms': 'mushroom', 'mushroom': 'mushroom',\n",
        "            'taquitos': 'taquito', 'taquito': 'taquito',\n",
        "            'batches': 'batch', 'batch': 'batch',\n",
        "            'patties': 'patty', 'patty': 'patty',\n",
        "            'sausages': 'sausage', 'sausage': 'sausage',\n",
        "            'tamales': 'tamale', 'tamale': 'tamale',\n",
        "            'lambs': 'lamb', 'lamb': 'lamb',\n",
        "            'cheeseburgers': 'cheeseburger', 'cheeseburger': 'cheeseburger',\n",
        "            'doughnuts': 'doughnut', 'doughnut': 'doughnut',\n",
        "            'hotdogs': 'hotdog', 'hotdog': 'hotdog',\n",
        "            'samosas': 'samosa', 'samosa': 'samosa',\n",
        "            'pies': 'pie', 'pie': 'pie',\n",
        "            'turkeys': 'turkey', 'turkey': 'turkey',\n",
        "            'chickens': 'chicken', 'chicken': 'chicken',\n",
        "            'potatoes': 'potato', 'potato': 'potato',\n",
        "            'peppers': 'pepper', 'pepper': 'pepper',\n",
        "            'eggplants': 'eggplant', 'eggplant': 'eggplant',\n",
        "            'fillets': 'fillet', 'fillet': 'fillet',\n",
        "            'squares': 'square', 'square': 'square',\n",
        "            'slabs': 'slab', 'slab': 'slab',\n",
        "            'wraps': 'wrap', 'wrap': 'wrap',\n",
        "            'burritos': 'burrito', 'burrito': 'burrito',\n",
        "            'slices': 'slice', 'slice': 'slice',\n",
        "        }\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        return X.apply(self.standardize_units)\n",
        "\n",
        "    def standardize_units(self, value):\n",
        "        try:\n",
        "            tokens = word_tokenize(value.lower())\n",
        "            standardized_tokens = [self.standard_units.get(token, token) for token in tokens]\n",
        "            standardized_value = ' '.join(standardized_tokens)\n",
        "            standardized_value = re.sub(r'(\\d+)\\s+(\\d+)/(\\d+)', lambda m: str(int(m.group(1)) + float(m.group(2)) / float(m.group(3))), standardized_value)\n",
        "            return standardized_value\n",
        "        except:\n",
        "            return 'None'\n",
        "\n",
        "# Custom transformer for tokenizing\n",
        "class CustomTokenizer(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        return X.apply(word_tokenize)\n",
        "\n",
        "# Custom transformer for encoding tokens\n",
        "class TokenEncoder(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        flat_tokens = list(chain.from_iterable(X))\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.encoded_tokens = self.label_encoder.fit_transform([str(token) for token in flat_tokens])\n",
        "        self.token_dict = dict(zip(flat_tokens, self.encoded_tokens.ravel()))\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        sequences = X.apply(lambda tokens: [self.token_dict[str(token)] for token in tokens])\n",
        "        # max_len = max(sequences.apply(len))\n",
        "        padded_sequences = pad_sequences(sequences, maxlen=2, padding='post')\n",
        "        return padded_sequences"
      ],
      "metadata": {
        "id": "eex9d6OPXfXj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model('/content/drive/MyDrive/recipes/recipe.keras')\n",
        "\n",
        "with open('/content/drive/MyDrive/recipes/pipeline.pkl', 'rb') as f:\n",
        "    pipeline = pickle.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/recipes/tokenizer.pkl', 'rb') as f:\n",
        "    tokenizer = pickle.load(f)"
      ],
      "metadata": {
        "id": "Gte4LzRrYlHf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\n",
        "    \"Name\": [\"Bourbon Chicken\"],\n",
        "    \"Description\": [\"\"],\n",
        "    \"Facts\": [\"\"],\n",
        "    \"Directions\": [\"\"],\n",
        "    \"Ingredients\": [\"\"],\n",
        "    \"Nutrition\": [\"\"],\n",
        "    \"Category\": [\"Chicken Breast\"],\n",
        "    \"Rating\": [4.58],\n",
        "    \"URL\": [\"https://www.food.com/recipe/bourbon-chicken-45809\"],\n",
        "    \"Fact_Ready In\": [35],\n",
        "    \"Fact_Yields\": [None],\n",
        "    \"Fact_Ingredients\": [11],\n",
        "    \"Fact_Serves\": [4],\n",
        "    \"Serving Size\": [330],\n",
        "    \"Servings Per Recipe\": [4],\n",
        "    \"Calories from Fat DV\": [42.0],\n",
        "    \"Calories from Fat\": [220000.0],\n",
        "    \"Total Fat DV\": [37.0],\n",
        "    \"Total Fat\": [24500.0],\n",
        "    \"Saturated Fat DV\": [32.0],\n",
        "    \"Saturated Fat\": [6500.0],\n",
        "    \"Cholesterol DV\": [48.0],\n",
        "    \"Cholesterol\": [145.3],\n",
        "    \"Sodium DV\": [65.0],\n",
        "    \"Sodium\": [1573.0],\n",
        "    \"Total Carbohydrate DV\": [7.0],\n",
        "    \"Total Carbohydrate\": [23400.0],\n",
        "    \"Dietary Fiber DV\": [1.0],\n",
        "    \"Dietary Fiber\": [300.0],\n",
        "    \"Sugars DV\": [85.0],\n",
        "    \"Sugars\": [21500.0],\n",
        "    \"Protein DV\": [100.0],\n",
        "    \"Protein\": [50100.0]\n",
        "}\n",
        "\n",
        "def generate_recipe(model, tokenizer, seed_text, num_features, max_sequence_len=250):\n",
        "  token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "  token_list = pad_sequences([token_list], maxlen=max_sequence_len, padding='pre')\n",
        "  generated_text = seed_text\n",
        "  last_word = \"\"\n",
        "  for _ in range(max_sequence_len):\n",
        "    # Make a prediction\n",
        "    predicted_probs = model.predict([token_list, token_list, num_features], verbose=0)\n",
        "\n",
        "    # Get the word with the highest probability\n",
        "    predicted_word_index = predicted_probs[:, -1, :].argmax(axis=-1)[0]\n",
        "\n",
        "    # Convert the index to a word\n",
        "    output_word = tokenizer.index_word.get(predicted_word_index, '')\n",
        "\n",
        "    # Break if end token is reached\n",
        "    if output_word == \"endseq\" or not output_word:\n",
        "      break\n",
        "\n",
        "    # Avoid repeating the same word consecutively\n",
        "    if output_word != last_word:\n",
        "      generated_text += ' ' + output_word\n",
        "      last_word = output_word\n",
        "\n",
        "    # Append the word to the seed text\n",
        "    seed_text += ' ' + output_word\n",
        "\n",
        "    # Update token_list for the next prediction\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "  # Post-processing to clean up repetitive phrases and make the text more coherent\n",
        "  generated_text = ' '.join(dict.fromkeys(generated_text.split()))\n",
        "  return generated_text\n",
        "\n",
        "# Generate a new recipe\n",
        "seed_text = \"chocolate cake with butter and honey\"\n",
        "num_features = pipeline.transform(pd.DataFrame(data))\n",
        "num_features = tf.convert_to_tensor(num_features)\n",
        "print(generate_recipe(model, tokenizer, seed_text, num_features))"
      ],
      "metadata": {
        "id": "eWKKshoYzRYW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "534e63c1-b86a-4c50-d324-f4de5a99322a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chocolate cake with butter and honey, heat until warm cream sugar in medium bowl. beat stir heat smooth a large mixture of butter and flour into proper boil. mix the mixture and store in cooler.\n"
          ]
        }
      ]
    }
  ]
}